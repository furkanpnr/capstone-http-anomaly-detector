# -*- coding: utf-8 -*-
"""model-fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15WbeHox3rbK0YsRPUNHasE8hJFKifGqe
"""

!pip install transformers datasets scikit-learn pandas

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
import torch
from datasets import Dataset

# Connec to the drive
from google.colab import drive
drive.mount('/content/drive')
# Paths
BASE_PATH = '/content/drive/MyDrive/'
DATA_SET_PATH = BASE_PATH + 'Capstone Project/Datasets/'
BALANCED_DATASET_PATH = DATA_SET_PATH + 'balanced_df_labeled.csv'

df = pd.read_csv(BALANCED_DATASET_PATH)

# URL ve Referrer
df["text"] = df["url"].fillna('') + " [SEP] " + df["referrer"].fillna('')

df = df[["text", "label"]]
df.head()

labels = df["label"].unique().tolist()
label2id = {label: idx for idx, label in enumerate(labels)}
id2label = {idx: label for label, idx in label2id.items()}

df["label"] = df["label"].map(label2id)

import json

with open('id2label.json', 'w') as f:
    f = json.dump(id2label, f)

print(id2label)
print(label2id)

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["text"].tolist(), df["label"].tolist(), test_size=0.2, stratify=df["label"]
)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(texts):
    return tokenizer(texts["text"], truncation=True)

train_dataset = Dataset.from_dict({"text": train_texts, "label": train_labels})
val_dataset = Dataset.from_dict({"text": val_texts, "label": val_labels})

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

preds = trainer.predict(val_dataset)
pred_labels = preds.predictions.argmax(-1)

print(classification_report(val_labels, pred_labels, target_names=label2id.keys()))

from transformers import BertTokenizer, BertForSequenceClassification

model_path = "/content/results/checkpoint-1515"
tokenizer = BertTokenizer.from_pretrained(model_path)
model = BertForSequenceClassification.from_pretrained(model_path)

test_samples = [
    # NORMAL
    {"url": "/index.html", "referrer": "https://www.google.com"},
    {"url": "/products/view?id=42", "referrer": "https://trustedpartner.com"},
    {"url": "/blog/python-tips", "referrer": "https://facebook.com"},

    # SQL INJECTION
    {"url": "/login?username=admin' OR '1'='1&password=1234", "referrer": "https://evil.com"},
    {"url": "/search?query=abc'; DROP TABLE users; --", "referrer": "https://attacker.site"},
    {"url": "/api/user?id=1%20UNION%20SELECT%20null,null--", "referrer": "https://test123.com"},

    # XSS
    {"url": "/comments?msg=<script>alert('Hacked!')</script>", "referrer": "https://someblog.com"},
    {"url": "/feedback?input=%3Cimg%20src%3Dx%20onerror%3Dalert('xss')%3E", "referrer": "https://weirdsite.net"},
    {"url": "/profile?bio=<svg/onload=alert(123)>", "referrer": "https://fake-social.com"},

    # PATH TRAVERSAL
    {"url": "/view?file=../../../../etc/passwd", "referrer": "https://testhost.com"},
    {"url": "/readme?doc=../../../boot.ini", "referrer": "https://some-ref.com"},
    {"url": "/log/download?name=..%2F..%2Fsecret.txt", "referrer": "https://trustedref.net"},

    # COMMAND INJECTION
    {"url": "/status?host=127.0.0.1; cat /etc/passwd", "referrer": "https://cmdtest.com"},
    {"url": "/ping?ip=8.8.8.8 && rm -rf /", "referrer": "https://admin-panel.biz"},
    {"url": "/execute?cmd=ls|whoami", "referrer": "https://danger-zone.com"}
]

texts = [sample["url"] + " [SEP] " + sample["referrer"] for sample in test_samples]

import torch

# Tokenize
inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)

# Run the Model
model.eval()
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=-1)

# id2label mapping
id2label = model.config.id2label

for i, pred in enumerate(predictions):
    print(f"Girdi: {texts[i]}")
    print(f"Tahmin Edilen Sınıf: {id2label[pred.item()]}")
    print("--------")

!zip -r model_checkpoint_1515.zip /content/results/checkpoint-1515

from google.colab import files
files.download("model_checkpoint_1515.zip")